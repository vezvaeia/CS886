{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreparationForTFT3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1-TAdLWsmxBFNocKh8CQSHLdOJJg3djRD",
      "authorship_tag": "ABX9TyP4Q8EhmmSlaZnzoz2xkuv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vezvaeia/CS886/blob/master/DataPreparationForTFT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbEqmVApz_64"
      },
      "source": [
        "# 1. Install and Import libraries and define parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nYK2n3Y0Ei9"
      },
      "source": [
        "### Install & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwUDsD-40hk0",
        "outputId": "a3b0ee05-3135-45a9-d7a6-d1b4da671bc5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-forecasting\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: pytorch-forecasting in /usr/local/lib/python3.7/dist-packages (0.8.4)\n",
            "Requirement already satisfied: fsspec<0.9.0,>=0.8.5 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (0.8.7)\n",
            "Requirement already satisfied: optuna<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (2.7.0)\n",
            "Requirement already satisfied: pytorch-lightning<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (1.2.8)\n",
            "Requirement already satisfied: torch<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (1.8.1+cu101)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn<0.25,>=0.23 in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (0.24.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-forecasting) (1.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec<0.9.0,>=0.8.5->pytorch-forecasting) (3.10.1)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (3.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (4.41.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (5.0.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.5.8)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.4.1)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (5.3.1)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.7->pytorch-forecasting) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pytorch-forecasting) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pytorch-forecasting) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pytorch-forecasting) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.23->pytorch-forecasting) (1.0.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pytorch-forecasting) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec<0.9.0,>=0.8.5->pytorch-forecasting) (3.4.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (2.1.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (5.5.1)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.5.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (3.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.0.0)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.1.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.28.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (54.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (0.2.5)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (0.4.4)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (20.3.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2020.12.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.4.8)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.2.8)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.1+cu101)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.8.7)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (54.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.10.1)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics>=0.2.0->pytorch_lightning) (20.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics>=0.2.0->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duysy8f6zlTc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import csv \n",
        "import random\n",
        "from transformers import BertTokenizer, RobertaTokenizer\n",
        "from transformers import BertForNextSentencePrediction, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from scipy import stats\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gc\n",
        "import itertools\n",
        "import nltk\n",
        "import transformers\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNlHSZJN_-lR"
      },
      "source": [
        "### Setting device, random seed, and runtime parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-DQtNqcAHPC",
        "outputId": "024259a7-df01-4053-b595-779535a96c8f"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Cuda available: \",torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Current device: \",  torch.cuda.current_device())\n",
        "\n",
        "seed = 204920\n",
        "seed2 = 293652\n",
        "\n",
        "random.seed(seed2)\n",
        "np.random.seed(seed2)\n",
        "torch.manual_seed(seed2)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available:  True\n",
            "Current device:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axiVHVrAQQib"
      },
      "source": [
        "### Setting Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxoS5ritQT2h",
        "outputId": "9b5a9f6d-69f6-4824-ae96-dabcf8bef615"
      },
      "source": [
        "COMMENT_DIMENSIONS = 20\n",
        "first_day = datetime.datetime(2020, 1, 1)\n",
        "last_day = datetime.datetime(2021, 2, 27)\n",
        "number_of_days = (last_day - first_day).days + 1\n",
        "print('Number of days: ', number_of_days)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of days:  424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRwiF8MRC7Qm"
      },
      "source": [
        "# 2. Reading and Processing Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X354BP30GCsp"
      },
      "source": [
        "### Moving datasets to Colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD87Mk5oF5hG"
      },
      "source": [
        "!cp  /content/drive/MyDrive/DL-project/Combined.xlsx Combined.xlsx\n",
        "!cp  /content/drive/MyDrive/DL-project/acaps_covid19_government_measures_dataset_0.xlsx acaps_covid19_government_measures_dataset_0.xlsx\n",
        "!cp /content/drive/MyDrive/DL-project/time_series_covid_19_confirmed_aggregated.csv time_series_covid_19_confirmed_aggregated.csv\n",
        "!cp /content/drive/MyDrive/DL-project/comment-embeddings.json comment-embeddings.json\n",
        "!cp /content/drive/MyDrive/DL-project/reduced-embeddings.json reduced-embeddings.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bATl96nKFkew"
      },
      "source": [
        "### Reading and processing time-series dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cHr9oV6GRVq",
        "outputId": "e693fd0b-e868-4f4c-82b8-a16f78d3396f"
      },
      "source": [
        "time_series_country_list = []\n",
        "time_series_date_list = [0]\n",
        "time_series_confirmed_cases = {}\n",
        "\n",
        "with open('time_series_covid_19_confirmed_aggregated.csv') as csvfile:    \n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    cnt = 0\n",
        "    \n",
        "    for row in reader:\n",
        "        if cnt == 0:\n",
        "            for i in range(1, len(row)):\n",
        "                datetimeObject = datetime.datetime.strptime(row[i], '%m/%d/%Y')\n",
        "                time_series_date_list.append((datetimeObject - first_day).days)\n",
        "        else:\n",
        "            country = row[0]\n",
        "            time_series_country_list.append(country)\n",
        "            time_series_confirmed_cases[country] = [0 for i in range(number_of_days)]\n",
        "\n",
        "            previous_accumulative_cases = 0\n",
        "            for i in range(1, len(row)):\n",
        "                time_series_confirmed_cases[country][time_series_date_list[i]] = int(row[i]) - previous_accumulative_cases\n",
        "                previous_accumulative_cases = int(row[i])\n",
        "\n",
        "        cnt += 1\n",
        "\n",
        "\n",
        "print('Countries: ', len(time_series_country_list), time_series_country_list)\n",
        "print('Dates: ', len(time_series_date_list), time_series_date_list)\n",
        "print('\\nConfirmed cases in China: \\n', time_series_confirmed_cases['China'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Countries:  201 ['Australia', 'Canada', 'China', 'Denmark', 'France', 'Netherlands', 'United Kingdom', 'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burma', 'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Central African Republic', 'Chad', 'Chile', 'Colombia', 'Comoros', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czechia', 'Denmark', 'Diamond Princess', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Holy See', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Korea, South', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'MS Zaandam', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Taiwan*', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'US', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'West Bank and Gaza', 'Yemen', 'Zambia', 'Zimbabwe', 'United States', 'Cape Verde', 'Czech Republic', 'Korea, Republic of', 'Myanmar']\n",
            "Dates:  404 [0, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]\n",
            "\n",
            "Confirmed cases in China: \n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 548, 95, 277, 486, 669, 802, 2632, 578, 2054, 1661, 2089, 4739, 3086, 3991, 3733, 3147, 3523, 2704, 3015, 2525, 2032, 373, 15136, 6463, 2055, 2100, 1921, 1777, 408, 458, 473, 1451, 21, 219, 513, 412, 434, 328, 428, 576, 204, 125, 125, 151, 153, 80, 53, 37, 27, 34, 11, 13, 32, 26, 30, 25, 44, 54, 94, 55, 130, 63, 93, 70, 121, 115, 102, 123, 76, 81, 82, 71, 79, 32, 59, 63, 53, 91, 74, 58, 73, 120, 79, 93, 50, 47, 357, 27, 18, 12, 36, 15, 16, 15, 10, 3, 6, 22, 4, 12, 3, 0, 5, 2, 2, 2, 5, 1, 14, 20, 1, 7, 6, 5, 9, 6, 10, 9, 0, 0, 0, 18, 3, 11, 7, 1, 3, 0, 17, 5, 18, 8, 7, -1, 11, 6, 9, 5, 4, 3, 11, 7, 12, 58, 49, 43, 44, 36, 36, 0, 59, 19, 52, 29, 20, 28, 24, 18, 14, 23, 5, 31, 14, 8, 19, 14, 18, 28, 33, 42, 0, 79, 46, 0, 109, 20, 81, 75, 16, 85, 119, 86, 198, 139, 157, 179, 189, 213, 207, 223, 276, 166, 172, 158, 114, 107, 122, 132, 120, 92, 121, 113, 52, 87, 99, 70, 65, 96, 66, 53, 33, 40, 49, 38, 41, 23, 34, 32, 30, 22, 27, 32, 19, 19, 20, 33, 22, 17, 33, 20, 9, 13, 27, 18, 23, 29, 22, 16, 18, 41, 17, 23, 35, 12, 18, 10, 15, 17, 15, 27, 22, 23, 17, 22, 17, 20, 25, 23, 15, 20, 41, 23, 27, 34, 18, 28, 11, 36, 20, 30, 17, 34, 16, 22, 29, 35, 20, 23, 24, 47, 49, 28, 40, 27, 31, 55, 26, 31, 43, 39, 31, 43, 28, 26, 33, 31, 24, 21, 22, 22, 13, 21, 29, 42, 60, 79, 95, 85, 106, 86, 98, 95, 133, 88, 91, 120, 108, 127, 119, 110, 93, 112, 116, 127, 99, 93, 111, 99, 110, 89, 108, 87, 132, 97, 100, 78, 70, 85, 77, 81, 91, 88, 77, 79, 87, 60, 63, 74, 86, 64, 88, 86, 78, 128, 134, 96, 175, 180, 173, 168, 159, 164, 225, 159, 221, 242, 100, 161, 200, 155, 139, 114, 91, 102, 120, 95, 64, 50, 49, 42, 50, 30, 41, 46, 40, 19, 33, 32, 19, 21, 25, 15, 27, 18, 21, 22, 31, 26, 23, 25, 19, 34, 39]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfUOMPJZLm9P"
      },
      "source": [
        "### Reading regulation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPaNoawre29O",
        "outputId": "e1b59c35-4387-481a-f8b1-db1db8fc73f1"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder as le\n",
        "\n",
        "xl_file = pd.ExcelFile('acaps_covid19_government_measures_dataset_0.xlsx')\n",
        "\n",
        "sheets = {sheet_name: xl_file.parse(sheet_name) \n",
        "          for sheet_name in xl_file.sheet_names}\n",
        "\n",
        "dataframe = sheets['Dataset']\n",
        "\n",
        "print('Sheets in the dataset:   ', sheets.keys())\n",
        "print('Number of regulations in the dataset: ', len(dataframe), '\\n')\n",
        "\n",
        "# print(dataframe.head())\n",
        "print('\\n', dataframe.info(), '\\n')\n",
        "\n",
        "dataset = {}\n",
        "for key in dataframe:\n",
        "    # dataframe[key] = le.fit_transform(dataframe[key].astype(str))\n",
        "    dataframe[key]=dataframe[key].astype('str')\n",
        "    dataset[key] = list(dataframe[key])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sheets in the dataset:    dict_keys(['About', 'Dataset', 'Dictionary'])\n",
            "Number of regulations in the dataset:  23923 \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23923 entries, 0 to 23922\n",
            "Data columns (total 18 columns):\n",
            " #   Column              Non-Null Count  Dtype         \n",
            "---  ------              --------------  -----         \n",
            " 0   ID                  23923 non-null  int64         \n",
            " 1   ISO                 23923 non-null  object        \n",
            " 2   COUNTRY             23923 non-null  object        \n",
            " 3   REGION              23923 non-null  object        \n",
            " 4   ADMIN_LEVEL_NAME    3682 non-null   object        \n",
            " 5   PCODE               0 non-null      float64       \n",
            " 6   LOG_TYPE            23923 non-null  object        \n",
            " 7   CATEGORY            23923 non-null  object        \n",
            " 8   MEASURE             23923 non-null  object        \n",
            " 9   TARGETED_POP_GROUP  7556 non-null   object        \n",
            " 10  COMMENTS            23799 non-null  object        \n",
            " 11  NON_COMPLIANCE      22764 non-null  object        \n",
            " 12  DATE_IMPLEMENTED    23630 non-null  datetime64[ns]\n",
            " 13  SOURCE              23900 non-null  object        \n",
            " 14  SOURCE_TYPE         23912 non-null  object        \n",
            " 15  LINK                23890 non-null  object        \n",
            " 16  ENTRY_DATE          23923 non-null  datetime64[ns]\n",
            " 17  Alternative source  1779 non-null   object        \n",
            "dtypes: datetime64[ns](2), float64(1), int64(1), object(14)\n",
            "memory usage: 3.3+ MB\n",
            "\n",
            " None \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1UWFoB5KhoL"
      },
      "source": [
        "### Processing regulation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Dv_MsFLzc6",
        "outputId": "24b565aa-75d8-4a16-8b14-955ff9e3de39"
      },
      "source": [
        "country_list = set([])\n",
        "\n",
        "for i in range(len(dataset['ID'])):\n",
        "    country = dataset['COUNTRY'][i]    \n",
        "    country_list.add(country)\n",
        "    \n",
        "country_list = list(country_list)\n",
        "country_list.sort()\n",
        "time_series_country_list.sort()\n",
        "\n",
        "print('Countries in regulation dataset: ', len(country_list), country_list)\n",
        "print('Countries in time-series dataset: ', len(time_series_country_list), time_series_country_list)\n",
        "\n",
        "common_countries = []\n",
        "for country in country_list:\n",
        "    if country in time_series_country_list:\n",
        "        common_countries.append(country)\n",
        "\n",
        "print('Number of countries in intersection of both datasets: ', len(common_countries), common_countries)\n",
        "\n",
        "regulations = {}\n",
        "\n",
        "for country in common_countries:\n",
        "    regulations[country] = [[] for i in range(number_of_days)]\n",
        "\n",
        "for i in range(len(dataset['ID'])):\n",
        "    \n",
        "    # Ignore the regulations with unknown implementation time\n",
        "    if dataset['DATE_IMPLEMENTED'][i] == 'NaT':\n",
        "        continue\n",
        "    \n",
        "    # Ignore the regulations that we don't have time-series of their countries\n",
        "    if dataset['COUNTRY'][i] not in common_countries:\n",
        "        continue\n",
        "    \n",
        "    # fill the empty comment field with ----\n",
        "    if dataset['COMMENTS'][i] == 'nan':\n",
        "        dataset['COMMENTS'][i] = '----'\n",
        "\n",
        "\n",
        "    country = dataset['COUNTRY'][i]\n",
        "    datetimeObject = datetime.datetime.strptime(dataset['DATE_IMPLEMENTED'][i], '%Y-%m-%d')\n",
        "    date = (datetimeObject - first_day).days\n",
        "\n",
        "    if dataset['LOG_TYPE'][i] == 'Introduction / extension of measures':\n",
        "        regulations[country][date].append(i)\n",
        "    else: # Phase-out measure \n",
        "        nothing = 0\n",
        "    \n",
        "print('\\nA sample of positive and negative regulation ids in Ghana: ')\n",
        "print(regulations['Ghana'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Countries in regulation dataset:  193 ['Afghanistan', 'Albania', 'Algeria', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'CAR', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', \"CÃ´te d'Ivoire\", 'DPRK', 'DRC', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'St. Kitts and Nevis', 'St. Lucia', 'St. Vincent and the Grenadines', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor Leste', 'Togo', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe']\n",
            "Countries in time-series dataset:  201 ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burma', 'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Czechia', 'Denmark', 'Denmark', 'Diamond Princess', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Holy See', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Korea, Republic of', 'Korea, South', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'MS Zaandam', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Taiwan*', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'US', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'West Bank and Gaza', 'Yemen', 'Zambia', 'Zimbabwe']\n",
            "Number of countries in intersection of both datasets:  175 ['Afghanistan', 'Albania', 'Algeria', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Tajikistan', 'Tanzania', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe']\n",
            "\n",
            "A sample of positive and negative regulation ids in Ghana: \n",
            "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [8615], [], [], [], [8616], [8617], [], [], [], [8618, 8619, 8620, 8621], [8622], [8623, 8624], [], [], [], [8625, 8626], [], [8627], [], [], [], [], [8628, 8629, 8630], [], [8631, 8632], [], [8633], [], [], [], [8634, 8635, 8636, 8637, 8638, 8639], [], [8640, 8641], [], [], [8642, 8643], [], [], [8644, 8645, 8646], [], [], [], [8647, 8648], [], [8649, 8650, 8651], [8654], [], [], [], [], [], [], [8655, 8656], [], [], [], [], [], [], [], [8657], [], [], [8658], [], [8659], [], [], [], [], [], [], [], [], [8660, 8661], [], [], [], [], [], [], [], [], [], [], [], [], [8662], [8663, 8664, 8665], [], [8666], [], [], [], [], [], [], [], [], [], [8670], [], [], [], [], [], [], [], [], [], [], [8673], [], [], [8674], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [8676], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [8682], [], [8684, 8685], [], [8686], [], [], [], [], [], [], [], [], [], [], [8687], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [8692, 8693], [], [], [], [], [], [], [], [8694, 8695, 8696], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jYxQVAdZQ6V"
      },
      "source": [
        "### Creating Measure and Category dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxF6NP7TZWE9",
        "outputId": "84543c7f-b097-40c9-d9af-3fbdcba39913"
      },
      "source": [
        "measure_dic = {}\n",
        "category_dic = {}\n",
        "\n",
        "for i in range(len(dataset['ID'])):\n",
        "    category = dataset['CATEGORY'][i]\n",
        "    measure = dataset['MEASURE'][i]\n",
        "\n",
        "    if category not in category_dic:\n",
        "        category_dic[category] = 0\n",
        "\n",
        "    if measure not in measure_dic:\n",
        "        measure_dic[measure] = 0\n",
        "    \n",
        "print(len(category_dic.keys()), category_dic)\n",
        "print(len(measure_dic.keys()), measure_dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 {'Public health measures': 0, 'Movement restrictions': 0, 'Governance and socio-economic measures': 0, 'Social distancing': 0, 'Lockdown': 0, 'Humanitarian exemption': 0}\n",
            "35 {'Awareness campaigns': 0, 'Health screenings in airports and border crossings': 0, 'International flights suspension': 0, 'Border checks': 0, 'Strengthening the public health system': 0, 'Isolation and quarantine policies': 0, 'Emergency administrative structures activated or established': 0, 'Surveillance and monitoring': 0, 'Other public health measures enforced': 0, 'Border closure': 0, 'General recommendations': 0, 'State of emergency declared': 0, 'Domestic travel restrictions': 0, 'Limit public gatherings': 0, 'Limit product imports/exports': 0, 'Schools closure': 0, 'Partial lockdown': 0, 'Changes in prison-related policies': 0, 'Closure of businesses and public services': 0, 'Checkpoints within the country': 0, 'Economic measures': 0, 'Military deployment': 0, 'Curfews': 0, 'Visa restrictions': 0, 'Requirement to wear protective gear in public': 0, 'Testing policy': 0, 'Amendments to funeral and burial regulations': 0, 'Full lockdown': 0, 'Lockdown of refugee/idp camps or other minorities': 0, 'Additional health/documents requirements upon arrival': 0, 'Mass population testing': 0, 'Humanitarian exemptions': 0, 'Psychological assistance and medical social work': 0, 'Complete border closure': 0, 'Obligatory medical tests not related to COVID-19': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX0cOhgw-6AA"
      },
      "source": [
        "### Creating country2region dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpWbgTDl_Cjv",
        "outputId": "5c800abb-8db7-4a11-82f8-80e43f0e2bfd"
      },
      "source": [
        "country2region = {}\n",
        "\n",
        "for i in range(len(dataset['ID'])):\n",
        "    country = dataset['COUNTRY'][i]\n",
        "    region = dataset['REGION'][i]\n",
        "\n",
        "    if country not in country2region:\n",
        "        country2region[country] = region\n",
        "\n",
        "print(country2region)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Afghanistan': 'Asia', 'Albania': 'Europe', 'Algeria': 'Africa', 'Angola': 'Africa', 'Antigua and Barbuda': 'Americas', 'Argentina': 'Americas', 'Armenia': 'Middle east', 'Australia': 'Pacific', 'Austria': 'Europe', 'Azerbaijan': 'Middle east', 'Bahamas': 'Americas', 'Bahrain': 'Middle east', 'Bangladesh': 'Asia', 'Barbados': 'Americas', 'Belarus': 'Europe', 'Belgium': 'Europe', 'Belize': 'Americas', 'Benin': 'Africa', 'Bhutan': 'Middle east', 'Bolivia': 'Americas', 'Bosnia and Herzegovina': 'Europe', 'Botswana': 'Africa', 'Brazil': 'Americas', 'Brunei': 'Middle east', 'Bulgaria': 'Europe', 'Burkina Faso': 'Africa', 'Burundi': 'Africa', \"CÃ´te d'Ivoire\": 'Africa', 'Cambodia': 'Asia', 'Cameroon': 'Africa', 'Canada': 'Americas', 'Cape Verde': 'Africa', 'CAR': 'Africa', 'Chad': 'Africa', 'Chile': 'Americas', 'China': 'Asia', 'Colombia': 'Americas', 'Comoros': 'Africa', 'Congo': 'Africa', 'Costa Rica': 'Americas', 'Croatia': 'Europe', 'Cuba': 'Americas', 'Cyprus': 'Europe', 'Czech Republic': 'Europe', 'Denmark': 'Europe', 'Djibouti': 'Africa', 'Dominica': 'Americas', 'Dominican Republic': 'Americas', 'DPRK': 'Asia', 'DRC': 'Africa', 'Ecuador': 'Americas', 'Egypt': 'Africa', 'El Salvador': 'Americas', 'Equatorial Guinea': 'Africa', 'Eritrea': 'Africa', 'Estonia': 'Europe', 'Eswatini': 'Africa', 'Ethiopia': 'Africa', 'Fiji': 'Pacific', 'Finland': 'Europe', 'France': 'Europe', 'Gabon': 'Africa', 'Gambia': 'Africa', 'Georgia': 'Europe', 'Germany': 'Europe', 'Ghana': 'Africa', 'Greece': 'Europe', 'Grenada': 'Americas', 'Guatemala': 'Americas', 'Guinea': 'Africa', 'Guinea-Bissau': 'Africa', 'Guyana': 'Americas', 'Haiti': 'Americas', 'Honduras': 'Americas', 'Hong Kong': 'Asia', 'Hungary': 'Europe', 'Iceland': 'Europe', 'India': 'Asia', 'Indonesia': 'Asia', 'Iran': 'Middle east', 'Iraq': 'Middle east', 'Ireland': 'Europe', 'Israel': 'Middle east', 'Italy': 'Europe', 'Jamaica': 'Americas', 'Japan': 'Asia', 'Jordan': 'Middle east', 'Kazakhstan': 'Asia', 'Kenya': 'Africa', 'Kiribati': 'Pacific', 'Korea, Republic of': 'Asia', 'Kuwait': 'Middle east', 'Kyrgyzstan': 'Asia', 'Laos': 'Asia', 'Latvia': 'Europe', 'Lebanon': 'Middle east', 'Lesotho': 'Africa', 'Liberia': 'Africa', 'Libya': 'Africa', 'Liechtenstein': 'Europe', 'Lithuania': 'Europe', 'Luxembourg': 'Europe', 'Macedonia': 'Europe', 'Madagascar': 'Africa', 'Malawi': 'Africa', 'Malaysia': 'Asia', 'Maldives': 'Asia', 'Mali': 'Africa', 'Malta': 'Europe', 'Marshall Islands': 'Pacific', 'Mauritania': 'Africa', 'Mauritius': 'Africa', 'Mexico': 'Americas', 'Micronesia': 'Pacific', 'Moldova': 'Europe', 'Mongolia': 'Asia', 'Montenegro': 'Europe', 'Morocco': 'Africa', 'Mozambique': 'Africa', 'Myanmar': 'Asia', 'Namibia': 'Africa', 'Nauru': 'Pacific', 'Nepal': 'Asia', 'Netherlands': 'Europe', 'New Zealand': 'Pacific', 'Nicaragua': 'Americas', 'Niger': 'Africa', 'Nigeria': 'Africa', 'Norway': 'Europe', 'Oman': 'Middle east', 'Pakistan': 'Asia', 'Palau': 'Pacific', 'Palestine': 'Middle east', 'Panama': 'Americas', 'Papua New Guinea': 'Pacific', 'Paraguay': 'Americas', 'Peru': 'Americas', 'Philippines': 'Asia', 'Poland': 'Europe', 'Portugal': 'Europe', 'Qatar': 'Middle east', 'Romania': 'Europe', 'Russia': 'Europe', 'Rwanda': 'Africa', 'Samoa': 'Pacific', 'San Marino': 'Europe', 'Sao Tome and Principe': 'Africa', 'Saudi Arabia': 'Middle east', 'Senegal': 'Africa', 'Serbia': 'Europe', 'Seychelles': 'Africa', 'Sierra Leone': 'Africa', 'Singapore': 'Asia', 'Slovakia': 'Europe', 'Slovenia': 'Europe', 'Solomon Islands': 'Pacific', 'Somalia': 'Africa', 'South Africa': 'Africa', 'South Sudan': 'Africa', 'Spain': 'Europe', 'Sri Lanka': 'Asia', 'St. Kitts and Nevis': 'Americas', 'St. Lucia': 'Americas', 'St. Vincent and the Grenadines': 'Americas', 'Sudan': 'Africa', 'Suriname': 'Americas', 'Sweden': 'Europe', 'Switzerland': 'Europe', 'Syria': 'Middle east', 'Tajikistan': 'Asia', 'Tanzania': 'Africa', 'Thailand': 'Asia', 'Timor Leste': 'Asia', 'Togo': 'Africa', 'Tonga': 'Pacific', 'Trinidad and Tobago': 'Americas', 'Tunisia': 'Africa', 'Turkey': 'Middle east', 'Turkmenistan': 'Asia', 'Tuvalu': 'Pacific', 'Uganda': 'Africa', 'Ukraine': 'Europe', 'United Arab Emirates': 'Middle east', 'United Kingdom': 'Europe', 'United States': 'Americas', 'Uruguay': 'Americas', 'Uzbekistan': 'Asia', 'Vanuatu': 'Pacific', 'Venezuela': 'Americas', 'Vietnam': 'Asia', 'Yemen': 'Middle east', 'Zambia': 'Africa', 'Zimbabwe': 'Africa'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcF4gVgLgx3C"
      },
      "source": [
        "# 3. Comment embeddings from DeBerta model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYYV72WgztS"
      },
      "source": [
        "### creating list of comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vF5nj8xi9CH",
        "outputId": "988db04b-8546-4042-ef7e-daa393e74e1c"
      },
      "source": [
        "comments = []\n",
        "comment_infos = []\n",
        "\n",
        "for country in regulations:\n",
        "    for day in range(len(regulations[country])):\n",
        "\n",
        "        if len(regulations[country][day]) == 0:\n",
        "            continue\n",
        "\n",
        "        comment = ''\n",
        "        for index in regulations[country][day]:\n",
        "            comment = comment + ' ' + dataset['COMMENTS'][index]\n",
        "            \n",
        "        comments.append(comment)\n",
        "        comment_infos.append((country, day))\n",
        "\n",
        "print('Number of comments: ', len(comments))\n",
        "print(comments[0:5])\n",
        "print(comment_infos[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of comments:  8794\n",
            "[' MoPH begins announcements on their facebook to make public aware of coronavirus. ', ' Health teams at airports will check passengers coming from China. ', ' Flights to China are suspended.  Health screenings of all passengers at airports. ', ' All China and Iran nationals', ' the ministry has prepared 100 bed to control this virus in Kabul and 200 others in the province hospital with all the facilities needed in the country. ----']\n",
            "[('Afghanistan', 23), ('Afghanistan', 25), ('Afghanistan', 26), ('Afghanistan', 31), ('Afghanistan', 32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ6aqRdgac4n"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzP53Wzsac4o"
      },
      "source": [
        "tokenizer = transformers.DebertaTokenizer.from_pretrained('microsoft/deberta-base') \n",
        "max_length = 256\n",
        "train_encodings = tokenizer(comments, add_special_tokens=True, return_token_type_ids=False, truncation=True, padding=True, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G5kUUQWac4p"
      },
      "source": [
        "### Creating pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crdsqJosac4p"
      },
      "source": [
        "class NSPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         if self.labels != None:\n",
        "#           item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.labels)\n",
        "        return len(self.encodings['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvcs4q22ac4q"
      },
      "source": [
        "train_dataset = NSPDataset(train_encodings)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06zqtx-rac4q"
      },
      "source": [
        "### Creating DeBerta Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKOTYuVwac4q",
        "outputId": "bd875c8d-3fad-45b9-d053-d63fbfe937e3"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = transformers.DebertaModel.from_pretrained('microsoft/deberta-base')\n",
        "\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  model = torch.nn.DataParallel(model)\n",
        "    \n",
        "model.to(device)\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DebertaModel(\n",
              "  (embeddings): DebertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "    (LayerNorm): DebertaLayerNorm()\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (encoder): DebertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (1): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (2): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (3): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (4): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (5): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (6): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (7): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (8): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (9): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (10): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (11): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (rel_embeddings): Embedding(1024, 768)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5eHPqgNac4r"
      },
      "source": [
        "### Generating Comment Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-6-aHVqtac4s",
        "outputId": "1330ea14-1658-43e9-cedd-4766d525054f"
      },
      "source": [
        "all_cls = []\n",
        "\n",
        "for iteration, batch in tqdm(enumerate(train_loader)):\n",
        "    \n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        last_hidden_state = model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        cls_tokens = last_hidden_state[:,0,:].detach()\n",
        "        \n",
        "        all_cls.append(cls_tokens)\n",
        "\n",
        "        \n",
        "# out_cls is a matrix of size number_of_not_null_comments (23799) X size_of_hidden_state_of_BERT (768)\n",
        "# In this matrix, for each comment we have an embedding vector.\n",
        "# Use \"ids\" list to map each comment with its ids.\n",
        "out_cls = torch.cat(all_cls, 0)\n",
        "\n",
        "print(\"shape of output matrix :\", out_cls.shape)\n",
        "comment_embeddings = out_cls.tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  3.12it/s]\u001b[A\n",
            "2it [00:00,  2.77it/s]\u001b[A\n",
            "3it [00:01,  2.58it/s]\u001b[A\n",
            "4it [00:01,  2.45it/s]\u001b[A\n",
            "5it [00:02,  2.36it/s]\u001b[A\n",
            "6it [00:02,  2.31it/s]\u001b[A\n",
            "7it [00:03,  2.27it/s]\u001b[A\n",
            "8it [00:03,  2.24it/s]\u001b[A\n",
            "9it [00:03,  2.23it/s]\u001b[A\n",
            "10it [00:04,  2.21it/s]\u001b[A\n",
            "11it [00:04,  2.21it/s]\u001b[A\n",
            "12it [00:05,  2.19it/s]\u001b[A\n",
            "13it [00:05,  2.20it/s]\u001b[A\n",
            "14it [00:06,  2.19it/s]\u001b[A\n",
            "15it [00:06,  2.19it/s]\u001b[A\n",
            "16it [00:07,  2.19it/s]\u001b[A\n",
            "17it [00:07,  2.19it/s]\u001b[A\n",
            "18it [00:08,  2.19it/s]\u001b[A\n",
            "19it [00:08,  2.19it/s]\u001b[A\n",
            "20it [00:09,  2.19it/s]\u001b[A\n",
            "21it [00:09,  2.19it/s]\u001b[A\n",
            "22it [00:09,  2.19it/s]\u001b[A\n",
            "23it [00:10,  2.19it/s]\u001b[A\n",
            "24it [00:10,  2.19it/s]\u001b[A\n",
            "25it [00:11,  2.19it/s]\u001b[A\n",
            "26it [00:11,  2.19it/s]\u001b[A\n",
            "27it [00:12,  2.19it/s]\u001b[A\n",
            "28it [00:12,  2.19it/s]\u001b[A\n",
            "29it [00:13,  2.19it/s]\u001b[A\n",
            "30it [00:13,  2.19it/s]\u001b[A\n",
            "31it [00:14,  2.19it/s]\u001b[A\n",
            "32it [00:14,  2.19it/s]\u001b[A\n",
            "33it [00:14,  2.19it/s]\u001b[A\n",
            "34it [00:15,  2.19it/s]\u001b[A\n",
            "35it [00:15,  2.19it/s]\u001b[A\n",
            "36it [00:16,  2.19it/s]\u001b[A\n",
            "37it [00:16,  2.19it/s]\u001b[A\n",
            "38it [00:17,  2.19it/s]\u001b[A\n",
            "39it [00:17,  2.19it/s]\u001b[A\n",
            "40it [00:18,  2.19it/s]\u001b[A\n",
            "41it [00:18,  2.19it/s]\u001b[A\n",
            "42it [00:19,  2.19it/s]\u001b[A\n",
            "43it [00:19,  2.19it/s]\u001b[A\n",
            "44it [00:19,  2.19it/s]\u001b[A\n",
            "45it [00:20,  2.19it/s]\u001b[A\n",
            "46it [00:20,  2.19it/s]\u001b[A\n",
            "47it [00:21,  2.19it/s]\u001b[A\n",
            "48it [00:21,  2.19it/s]\u001b[A\n",
            "49it [00:22,  2.19it/s]\u001b[A\n",
            "50it [00:22,  2.19it/s]\u001b[A\n",
            "51it [00:23,  2.19it/s]\u001b[A\n",
            "52it [00:23,  2.19it/s]\u001b[A\n",
            "53it [00:24,  2.19it/s]\u001b[A\n",
            "54it [00:24,  2.19it/s]\u001b[A\n",
            "55it [00:25,  2.19it/s]\u001b[A\n",
            "56it [00:25,  2.19it/s]\u001b[A\n",
            "57it [00:25,  2.19it/s]\u001b[A\n",
            "58it [00:26,  2.19it/s]\u001b[A\n",
            "59it [00:26,  2.19it/s]\u001b[A\n",
            "60it [00:27,  2.19it/s]\u001b[A\n",
            "61it [00:27,  2.19it/s]\u001b[A\n",
            "62it [00:28,  2.19it/s]\u001b[A\n",
            "63it [00:28,  2.19it/s]\u001b[A\n",
            "64it [00:29,  2.19it/s]\u001b[A\n",
            "65it [00:29,  2.19it/s]\u001b[A\n",
            "66it [00:30,  2.19it/s]\u001b[A\n",
            "67it [00:30,  2.18it/s]\u001b[A\n",
            "68it [00:30,  2.18it/s]\u001b[A\n",
            "69it [00:31,  2.18it/s]\u001b[A\n",
            "70it [00:31,  2.18it/s]\u001b[A\n",
            "71it [00:32,  2.18it/s]\u001b[A\n",
            "72it [00:32,  2.18it/s]\u001b[A\n",
            "73it [00:33,  2.18it/s]\u001b[A\n",
            "74it [00:33,  2.18it/s]\u001b[A\n",
            "75it [00:34,  2.19it/s]\u001b[A\n",
            "76it [00:34,  2.19it/s]\u001b[A\n",
            "77it [00:35,  2.19it/s]\u001b[A\n",
            "78it [00:35,  2.18it/s]\u001b[A\n",
            "79it [00:35,  2.18it/s]\u001b[A\n",
            "80it [00:36,  2.18it/s]\u001b[A\n",
            "81it [00:36,  2.18it/s]\u001b[A\n",
            "82it [00:37,  2.18it/s]\u001b[A\n",
            "83it [00:37,  2.19it/s]\u001b[A\n",
            "84it [00:38,  2.19it/s]\u001b[A\n",
            "85it [00:38,  2.19it/s]\u001b[A\n",
            "86it [00:39,  2.19it/s]\u001b[A\n",
            "87it [00:39,  2.19it/s]\u001b[A\n",
            "88it [00:40,  2.19it/s]\u001b[A\n",
            "89it [00:40,  2.19it/s]\u001b[A\n",
            "90it [00:41,  2.19it/s]\u001b[A\n",
            "91it [00:41,  2.18it/s]\u001b[A\n",
            "92it [00:41,  2.18it/s]\u001b[A\n",
            "93it [00:42,  2.18it/s]\u001b[A\n",
            "94it [00:42,  2.18it/s]\u001b[A\n",
            "95it [00:43,  2.18it/s]\u001b[A\n",
            "96it [00:43,  2.19it/s]\u001b[A\n",
            "97it [00:44,  2.18it/s]\u001b[A\n",
            "98it [00:44,  2.19it/s]\u001b[A\n",
            "99it [00:45,  2.18it/s]\u001b[A\n",
            "100it [00:45,  2.18it/s]\u001b[A\n",
            "101it [00:46,  2.19it/s]\u001b[A\n",
            "102it [00:46,  2.18it/s]\u001b[A\n",
            "103it [00:46,  2.19it/s]\u001b[A\n",
            "104it [00:47,  2.18it/s]\u001b[A\n",
            "105it [00:47,  2.18it/s]\u001b[A\n",
            "106it [00:48,  2.19it/s]\u001b[A\n",
            "107it [00:48,  2.18it/s]\u001b[A\n",
            "108it [00:49,  2.18it/s]\u001b[A\n",
            "109it [00:49,  2.18it/s]\u001b[A\n",
            "110it [00:50,  2.18it/s]\u001b[A\n",
            "111it [00:50,  2.18it/s]\u001b[A\n",
            "112it [00:51,  2.18it/s]\u001b[A\n",
            "113it [00:51,  2.18it/s]\u001b[A\n",
            "114it [00:52,  2.18it/s]\u001b[A\n",
            "115it [00:52,  2.18it/s]\u001b[A\n",
            "116it [00:52,  2.18it/s]\u001b[A\n",
            "117it [00:53,  2.19it/s]\u001b[A\n",
            "118it [00:53,  2.18it/s]\u001b[A\n",
            "119it [00:54,  2.18it/s]\u001b[A\n",
            "120it [00:54,  2.19it/s]\u001b[A\n",
            "121it [00:55,  2.19it/s]\u001b[A\n",
            "122it [00:55,  2.19it/s]\u001b[A\n",
            "123it [00:56,  2.19it/s]\u001b[A\n",
            "124it [00:56,  2.18it/s]\u001b[A\n",
            "125it [00:57,  2.19it/s]\u001b[A\n",
            "126it [00:57,  2.19it/s]\u001b[A\n",
            "127it [00:57,  2.19it/s]\u001b[A\n",
            "128it [00:58,  2.19it/s]\u001b[A\n",
            "129it [00:58,  2.19it/s]\u001b[A\n",
            "130it [00:59,  2.18it/s]\u001b[A\n",
            "131it [00:59,  2.18it/s]\u001b[A\n",
            "132it [01:00,  2.18it/s]\u001b[A\n",
            "133it [01:00,  2.18it/s]\u001b[A\n",
            "134it [01:01,  2.18it/s]\u001b[A\n",
            "135it [01:01,  2.18it/s]\u001b[A\n",
            "136it [01:02,  2.18it/s]\u001b[A\n",
            "137it [01:02,  2.16it/s]\u001b[A\n",
            "138it [01:03,  2.19it/s]\u001b[A\n",
            "139it [01:03,  2.19it/s]\u001b[A\n",
            "140it [01:03,  2.19it/s]\u001b[A\n",
            "141it [01:04,  2.19it/s]\u001b[A\n",
            "142it [01:04,  2.18it/s]\u001b[A\n",
            "143it [01:05,  2.18it/s]\u001b[A\n",
            "144it [01:05,  2.18it/s]\u001b[A\n",
            "145it [01:06,  2.18it/s]\u001b[A\n",
            "146it [01:06,  2.18it/s]\u001b[A\n",
            "147it [01:07,  2.18it/s]\u001b[A\n",
            "148it [01:07,  2.19it/s]\u001b[A\n",
            "149it [01:08,  2.19it/s]\u001b[A\n",
            "150it [01:08,  2.19it/s]\u001b[A\n",
            "151it [01:08,  2.18it/s]\u001b[A\n",
            "152it [01:09,  2.18it/s]\u001b[A\n",
            "153it [01:09,  2.18it/s]\u001b[A\n",
            "154it [01:10,  2.18it/s]\u001b[A\n",
            "155it [01:10,  2.18it/s]\u001b[A\n",
            "156it [01:11,  2.18it/s]\u001b[A\n",
            "157it [01:11,  2.18it/s]\u001b[A\n",
            "158it [01:12,  2.18it/s]\u001b[A\n",
            "159it [01:12,  2.17it/s]\u001b[A\n",
            "160it [01:13,  2.18it/s]\u001b[A\n",
            "161it [01:13,  2.18it/s]\u001b[A\n",
            "162it [01:13,  2.18it/s]\u001b[A\n",
            "163it [01:14,  2.18it/s]\u001b[A\n",
            "164it [01:14,  2.18it/s]\u001b[A\n",
            "165it [01:15,  2.18it/s]\u001b[A\n",
            "166it [01:15,  2.18it/s]\u001b[A\n",
            "167it [01:16,  2.18it/s]\u001b[A\n",
            "168it [01:16,  2.18it/s]\u001b[A\n",
            "169it [01:17,  2.18it/s]\u001b[A\n",
            "170it [01:17,  2.18it/s]\u001b[A\n",
            "171it [01:18,  2.18it/s]\u001b[A\n",
            "172it [01:18,  2.17it/s]\u001b[A\n",
            "173it [01:19,  2.18it/s]\u001b[A\n",
            "174it [01:19,  2.18it/s]\u001b[A\n",
            "175it [01:19,  2.18it/s]\u001b[A\n",
            "176it [01:20,  2.18it/s]\u001b[A\n",
            "177it [01:20,  2.17it/s]\u001b[A\n",
            "178it [01:21,  2.18it/s]\u001b[A\n",
            "179it [01:21,  2.18it/s]\u001b[A\n",
            "180it [01:22,  2.18it/s]\u001b[A\n",
            "181it [01:22,  2.18it/s]\u001b[A\n",
            "182it [01:23,  2.17it/s]\u001b[A\n",
            "183it [01:23,  2.17it/s]\u001b[A\n",
            "184it [01:24,  2.17it/s]\u001b[A\n",
            "185it [01:24,  2.17it/s]\u001b[A\n",
            "186it [01:25,  2.17it/s]\u001b[A\n",
            "187it [01:25,  2.17it/s]\u001b[A\n",
            "188it [01:25,  2.17it/s]\u001b[A\n",
            "189it [01:26,  2.16it/s]\u001b[A\n",
            "190it [01:26,  2.17it/s]\u001b[A\n",
            "191it [01:27,  2.18it/s]\u001b[A\n",
            "192it [01:27,  2.17it/s]\u001b[A\n",
            "193it [01:28,  2.17it/s]\u001b[A\n",
            "194it [01:28,  2.18it/s]\u001b[A\n",
            "195it [01:29,  2.17it/s]\u001b[A\n",
            "196it [01:29,  2.17it/s]\u001b[A\n",
            "197it [01:30,  2.17it/s]\u001b[A\n",
            "198it [01:30,  2.17it/s]\u001b[A\n",
            "199it [01:31,  2.17it/s]\u001b[A\n",
            "200it [01:31,  2.16it/s]\u001b[A\n",
            "201it [01:31,  2.17it/s]\u001b[A\n",
            "202it [01:32,  2.17it/s]\u001b[A\n",
            "203it [01:32,  2.17it/s]\u001b[A\n",
            "204it [01:33,  2.16it/s]\u001b[A\n",
            "205it [01:33,  2.16it/s]\u001b[A\n",
            "206it [01:34,  2.16it/s]\u001b[A\n",
            "207it [01:34,  2.16it/s]\u001b[A\n",
            "208it [01:35,  2.16it/s]\u001b[A\n",
            "209it [01:35,  2.17it/s]\u001b[A\n",
            "210it [01:36,  2.16it/s]\u001b[A\n",
            "211it [01:36,  2.16it/s]\u001b[A\n",
            "212it [01:37,  2.16it/s]\u001b[A\n",
            "213it [01:37,  2.16it/s]\u001b[A\n",
            "214it [01:37,  2.16it/s]\u001b[A\n",
            "215it [01:38,  2.16it/s]\u001b[A\n",
            "216it [01:38,  2.17it/s]\u001b[A\n",
            "217it [01:39,  2.17it/s]\u001b[A\n",
            "218it [01:39,  2.17it/s]\u001b[A\n",
            "219it [01:40,  2.17it/s]\u001b[A\n",
            "220it [01:40,  2.17it/s]\u001b[A\n",
            "221it [01:41,  2.17it/s]\u001b[A\n",
            "222it [01:41,  2.17it/s]\u001b[A\n",
            "223it [01:42,  2.16it/s]\u001b[A\n",
            "224it [01:42,  2.16it/s]\u001b[A\n",
            "225it [01:43,  2.16it/s]\u001b[A\n",
            "226it [01:43,  2.17it/s]\u001b[A\n",
            "227it [01:43,  2.16it/s]\u001b[A\n",
            "228it [01:44,  2.17it/s]\u001b[A\n",
            "229it [01:44,  2.17it/s]\u001b[A\n",
            "230it [01:45,  2.16it/s]\u001b[A\n",
            "231it [01:45,  2.16it/s]\u001b[A\n",
            "232it [01:46,  2.16it/s]\u001b[A\n",
            "233it [01:46,  2.16it/s]\u001b[A\n",
            "234it [01:47,  2.16it/s]\u001b[A\n",
            "235it [01:47,  2.17it/s]\u001b[A\n",
            "236it [01:48,  2.16it/s]\u001b[A\n",
            "237it [01:48,  2.16it/s]\u001b[A\n",
            "238it [01:49,  2.16it/s]\u001b[A\n",
            "239it [01:49,  2.16it/s]\u001b[A\n",
            "240it [01:49,  2.16it/s]\u001b[A\n",
            "241it [01:50,  2.16it/s]\u001b[A\n",
            "242it [01:50,  2.16it/s]\u001b[A\n",
            "243it [01:51,  2.16it/s]\u001b[A\n",
            "244it [01:51,  2.15it/s]\u001b[A\n",
            "245it [01:52,  2.15it/s]\u001b[A\n",
            "246it [01:52,  2.16it/s]\u001b[A\n",
            "247it [01:53,  2.15it/s]\u001b[A\n",
            "248it [01:53,  2.15it/s]\u001b[A\n",
            "249it [01:54,  2.15it/s]\u001b[A\n",
            "250it [01:54,  2.15it/s]\u001b[A\n",
            "251it [01:55,  2.15it/s]\u001b[A\n",
            "252it [01:55,  2.15it/s]\u001b[A\n",
            "253it [01:56,  2.15it/s]\u001b[A\n",
            "254it [01:56,  2.15it/s]\u001b[A\n",
            "255it [01:56,  2.16it/s]\u001b[A\n",
            "256it [01:57,  2.16it/s]\u001b[A\n",
            "257it [01:57,  2.15it/s]\u001b[A\n",
            "258it [01:58,  2.16it/s]\u001b[A\n",
            "259it [01:58,  2.15it/s]\u001b[A\n",
            "260it [01:59,  2.15it/s]\u001b[A\n",
            "261it [01:59,  2.15it/s]\u001b[A\n",
            "262it [02:00,  2.15it/s]\u001b[A\n",
            "263it [02:00,  2.15it/s]\u001b[A\n",
            "264it [02:01,  2.15it/s]\u001b[A\n",
            "265it [02:01,  2.15it/s]\u001b[A\n",
            "266it [02:02,  2.15it/s]\u001b[A\n",
            "267it [02:02,  2.15it/s]\u001b[A\n",
            "268it [02:02,  2.15it/s]\u001b[A\n",
            "269it [02:03,  2.15it/s]\u001b[A\n",
            "270it [02:03,  2.14it/s]\u001b[A\n",
            "271it [02:04,  2.15it/s]\u001b[A\n",
            "272it [02:04,  2.15it/s]\u001b[A\n",
            "273it [02:05,  2.15it/s]\u001b[A\n",
            "274it [02:05,  2.15it/s]\u001b[A\n",
            "275it [02:06,  2.15it/s]\u001b[A\n",
            "276it [02:06,  2.14it/s]\u001b[A\n",
            "277it [02:07,  2.14it/s]\u001b[A\n",
            "278it [02:07,  2.14it/s]\u001b[A\n",
            "279it [02:08,  2.14it/s]\u001b[A\n",
            "280it [02:08,  2.15it/s]\u001b[A\n",
            "281it [02:09,  2.14it/s]\u001b[A\n",
            "282it [02:09,  2.14it/s]\u001b[A\n",
            "283it [02:09,  2.15it/s]\u001b[A\n",
            "284it [02:10,  2.15it/s]\u001b[A\n",
            "285it [02:10,  2.15it/s]\u001b[A\n",
            "286it [02:11,  2.15it/s]\u001b[A\n",
            "287it [02:11,  2.14it/s]\u001b[A\n",
            "288it [02:12,  2.15it/s]\u001b[A\n",
            "289it [02:12,  2.14it/s]\u001b[A\n",
            "290it [02:13,  2.14it/s]\u001b[A\n",
            "291it [02:13,  2.15it/s]\u001b[A\n",
            "292it [02:14,  2.14it/s]\u001b[A\n",
            "293it [02:14,  2.14it/s]\u001b[A\n",
            "294it [02:15,  2.14it/s]\u001b[A\n",
            "295it [02:15,  2.14it/s]\u001b[A\n",
            "296it [02:16,  2.14it/s]\u001b[A\n",
            "297it [02:16,  2.14it/s]\u001b[A\n",
            "298it [02:16,  2.14it/s]\u001b[A\n",
            "299it [02:17,  2.14it/s]\u001b[A\n",
            "300it [02:17,  2.14it/s]\u001b[A\n",
            "301it [02:18,  2.14it/s]\u001b[A\n",
            "302it [02:18,  2.14it/s]\u001b[A\n",
            "303it [02:19,  2.14it/s]\u001b[A\n",
            "304it [02:19,  2.14it/s]\u001b[A\n",
            "305it [02:20,  2.14it/s]\u001b[A\n",
            "306it [02:20,  2.14it/s]\u001b[A\n",
            "307it [02:21,  2.14it/s]\u001b[A\n",
            "308it [02:21,  2.14it/s]\u001b[A\n",
            "309it [02:22,  2.14it/s]\u001b[A\n",
            "310it [02:22,  2.14it/s]\u001b[A\n",
            "311it [02:23,  2.14it/s]\u001b[A\n",
            "312it [02:23,  2.14it/s]\u001b[A\n",
            "313it [02:23,  2.14it/s]\u001b[A\n",
            "314it [02:24,  2.14it/s]\u001b[A\n",
            "315it [02:24,  2.14it/s]\u001b[A\n",
            "316it [02:25,  2.13it/s]\u001b[A\n",
            "317it [02:25,  2.14it/s]\u001b[A\n",
            "318it [02:26,  2.14it/s]\u001b[A\n",
            "319it [02:26,  2.14it/s]\u001b[A\n",
            "320it [02:27,  2.14it/s]\u001b[A\n",
            "321it [02:27,  2.14it/s]\u001b[A\n",
            "322it [02:28,  2.14it/s]\u001b[A\n",
            "323it [02:28,  2.14it/s]\u001b[A\n",
            "324it [02:29,  2.13it/s]\u001b[A\n",
            "325it [02:29,  2.13it/s]\u001b[A\n",
            "326it [02:30,  2.14it/s]\u001b[A\n",
            "327it [02:30,  2.14it/s]\u001b[A\n",
            "328it [02:31,  2.14it/s]\u001b[A\n",
            "329it [02:31,  2.13it/s]\u001b[A\n",
            "330it [02:31,  2.13it/s]\u001b[A\n",
            "331it [02:32,  2.13it/s]\u001b[A\n",
            "332it [02:32,  2.13it/s]\u001b[A\n",
            "333it [02:33,  2.14it/s]\u001b[A\n",
            "334it [02:33,  2.13it/s]\u001b[A\n",
            "335it [02:34,  2.14it/s]\u001b[A\n",
            "336it [02:34,  2.14it/s]\u001b[A\n",
            "337it [02:35,  2.14it/s]\u001b[A\n",
            "338it [02:35,  2.13it/s]\u001b[A\n",
            "339it [02:36,  2.13it/s]\u001b[A\n",
            "340it [02:36,  2.14it/s]\u001b[A\n",
            "341it [02:37,  2.14it/s]\u001b[A\n",
            "342it [02:37,  2.14it/s]\u001b[A\n",
            "343it [02:38,  2.14it/s]\u001b[A\n",
            "344it [02:38,  2.13it/s]\u001b[A\n",
            "345it [02:38,  2.14it/s]\u001b[A\n",
            "346it [02:39,  2.14it/s]\u001b[A\n",
            "347it [02:39,  2.13it/s]\u001b[A\n",
            "348it [02:40,  2.14it/s]\u001b[A\n",
            "349it [02:40,  2.13it/s]\u001b[A\n",
            "350it [02:41,  2.13it/s]\u001b[A\n",
            "351it [02:41,  2.13it/s]\u001b[A\n",
            "352it [02:42,  2.13it/s]\u001b[A\n",
            "353it [02:42,  2.13it/s]\u001b[A\n",
            "354it [02:43,  2.13it/s]\u001b[A\n",
            "355it [02:43,  2.13it/s]\u001b[A\n",
            "356it [02:44,  2.13it/s]\u001b[A\n",
            "357it [02:44,  2.13it/s]\u001b[A\n",
            "358it [02:45,  2.13it/s]\u001b[A\n",
            "359it [02:45,  2.13it/s]\u001b[A\n",
            "360it [02:46,  2.13it/s]\u001b[A\n",
            "361it [02:46,  2.13it/s]\u001b[A\n",
            "362it [02:46,  2.13it/s]\u001b[A\n",
            "363it [02:47,  2.14it/s]\u001b[A\n",
            "364it [02:47,  2.13it/s]\u001b[A\n",
            "365it [02:48,  2.13it/s]\u001b[A\n",
            "366it [02:48,  2.14it/s]\u001b[A\n",
            "367it [02:49,  2.14it/s]\u001b[A\n",
            "368it [02:49,  2.14it/s]\u001b[A\n",
            "369it [02:50,  2.14it/s]\u001b[A\n",
            "370it [02:50,  2.14it/s]\u001b[A\n",
            "371it [02:51,  2.14it/s]\u001b[A\n",
            "372it [02:51,  2.13it/s]\u001b[A\n",
            "373it [02:52,  2.13it/s]\u001b[A\n",
            "374it [02:52,  2.13it/s]\u001b[A\n",
            "375it [02:53,  2.13it/s]\u001b[A\n",
            "376it [02:53,  2.13it/s]\u001b[A\n",
            "377it [02:53,  2.13it/s]\u001b[A\n",
            "378it [02:54,  2.13it/s]\u001b[A\n",
            "379it [02:54,  2.13it/s]\u001b[A\n",
            "380it [02:55,  2.13it/s]\u001b[A\n",
            "381it [02:55,  2.13it/s]\u001b[A\n",
            "382it [02:56,  2.13it/s]\u001b[A\n",
            "383it [02:56,  2.13it/s]\u001b[A\n",
            "384it [02:57,  2.13it/s]\u001b[A\n",
            "385it [02:57,  2.13it/s]\u001b[A\n",
            "386it [02:58,  2.13it/s]\u001b[A\n",
            "387it [02:58,  2.13it/s]\u001b[A\n",
            "388it [02:59,  2.13it/s]\u001b[A\n",
            "389it [02:59,  2.13it/s]\u001b[A\n",
            "390it [03:00,  2.13it/s]\u001b[A\n",
            "391it [03:00,  2.14it/s]\u001b[A\n",
            "392it [03:01,  2.14it/s]\u001b[A\n",
            "393it [03:01,  2.14it/s]\u001b[A\n",
            "394it [03:01,  2.13it/s]\u001b[A\n",
            "395it [03:02,  2.13it/s]\u001b[A\n",
            "396it [03:02,  2.13it/s]\u001b[A\n",
            "397it [03:03,  2.13it/s]\u001b[A\n",
            "398it [03:03,  2.13it/s]\u001b[A\n",
            "399it [03:04,  2.14it/s]\u001b[A\n",
            "400it [03:04,  2.14it/s]\u001b[A\n",
            "401it [03:05,  2.13it/s]\u001b[A\n",
            "402it [03:05,  2.14it/s]\u001b[A\n",
            "403it [03:06,  2.15it/s]\u001b[A\n",
            "404it [03:06,  2.15it/s]\u001b[A\n",
            "405it [03:07,  2.14it/s]\u001b[A\n",
            "406it [03:07,  2.14it/s]\u001b[A\n",
            "407it [03:08,  2.14it/s]\u001b[A\n",
            "408it [03:08,  2.14it/s]\u001b[A\n",
            "409it [03:08,  2.14it/s]\u001b[A\n",
            "410it [03:09,  2.14it/s]\u001b[A\n",
            "411it [03:09,  2.14it/s]\u001b[A\n",
            "412it [03:10,  2.14it/s]\u001b[A\n",
            "413it [03:10,  2.14it/s]\u001b[A\n",
            "414it [03:11,  2.13it/s]\u001b[A\n",
            "415it [03:11,  2.14it/s]\u001b[A\n",
            "416it [03:12,  2.13it/s]\u001b[A\n",
            "417it [03:12,  2.12it/s]\u001b[A\n",
            "418it [03:13,  2.14it/s]\u001b[A\n",
            "419it [03:13,  2.14it/s]\u001b[A\n",
            "420it [03:14,  2.14it/s]\u001b[A\n",
            "421it [03:14,  2.14it/s]\u001b[A\n",
            "422it [03:15,  2.14it/s]\u001b[A\n",
            "423it [03:15,  2.14it/s]\u001b[A\n",
            "424it [03:15,  2.14it/s]\u001b[A\n",
            "425it [03:16,  2.14it/s]\u001b[A\n",
            "426it [03:16,  2.14it/s]\u001b[A\n",
            "427it [03:17,  2.14it/s]\u001b[A\n",
            "428it [03:17,  2.14it/s]\u001b[A\n",
            "429it [03:18,  2.14it/s]\u001b[A\n",
            "430it [03:18,  2.14it/s]\u001b[A\n",
            "431it [03:19,  2.14it/s]\u001b[A\n",
            "432it [03:19,  2.15it/s]\u001b[A\n",
            "433it [03:20,  2.14it/s]\u001b[A\n",
            "434it [03:20,  2.14it/s]\u001b[A\n",
            "435it [03:21,  2.14it/s]\u001b[A\n",
            "436it [03:21,  2.15it/s]\u001b[A\n",
            "437it [03:22,  2.14it/s]\u001b[A\n",
            "438it [03:22,  2.14it/s]\u001b[A\n",
            "439it [03:22,  2.14it/s]\u001b[A\n",
            "440it [03:23,  2.14it/s]\u001b[A\n",
            "441it [03:23,  2.14it/s]\u001b[A\n",
            "442it [03:24,  2.14it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-50e7f85d5b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         )\n\u001b[1;32m    947\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m             )\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         )\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_att\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_att\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mrel_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisentangled_att_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrel_att\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mdisentangled_att_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mp2c_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_query_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             p2c_att = torch.gather(\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0mp2c_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp2c_dynamic_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2c_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             ).transpose(-1, -2)\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.43 GiB total capacity; 6.45 GiB already allocated; 32.81 MiB free; 6.70 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2fpt1LplQGZ"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IxTKXb_Tu7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "6dc9444b-7096-4286-a358-198bea2dca8e"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from plotly import graph_objs as go\n",
        "import random\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "a = kmeans.fit(np.array(comment_embeddings))\n",
        "centroids = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "sample_indexes = []\n",
        "for i in range(len(centroids)):\n",
        "    all_label_indexes = []\n",
        "    for j in range(len(labels)):\n",
        "        if labels[j] == i:\n",
        "            all_label_indexes.append(j)\n",
        "            \n",
        "    SAMPLE_NUM = min(100, len(all_label_indexes))\n",
        "    sample_label_indexes = random.sample(all_label_indexes, SAMPLE_NUM)\n",
        "    sample_indexes = sample_indexes + sample_label_indexes\n",
        "\n",
        "sample_embeddings = []\n",
        "sample_labels = []\n",
        "sample_ids = []\n",
        "\n",
        "for index in sample_indexes:\n",
        "    sample_embeddings.append(comment_embeddings[index])\n",
        "    sample_labels.append(labels[index])\n",
        "    sample_ids.append(not_null_ids[index])\n",
        "\n",
        "reduced_embeddings = TSNE(verbose=2).fit_transform(sample_embeddings)\n",
        "\n",
        "colors = []\n",
        "for i in range(len(sample_labels)):\n",
        "    colors.append('hsl(' + str(sample_labels[i]* (360/len(centroids))) + ',50%' + ',50%)')\n",
        "    \n",
        "print(len(reduced_embeddings))\n",
        "\n",
        "fig = go.Figure(go.Scattergl(x=reduced_embeddings[:,0],\n",
        "                    y=reduced_embeddings[:,1],\n",
        "                    mode='markers',\n",
        "                    marker={'color':colors},\n",
        "                    text=[(sample_ids[x], sample_labels[x]) for x in range(len(reduced_embeddings))]))\n",
        "\n",
        "configure_plotly_browser_state()\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-67aba7b09416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comment_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Q2hNAeLFsJ"
      },
      "source": [
        "### Reduce embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iorXkPEe2oIY",
        "outputId": "aee6b0ee-e43b-4e05-82fd-308bbe3cf099"
      },
      "source": [
        "reduced_embeddings = comment_embeddings\n",
        "print(labels[0:10], len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11048 20 [7.845292310613456, 4.628665314575266, 0.8785070598723065, -1.7030493792400954, 0.7023583855166133, 0.25813270612300493, -0.18706157316442965, -0.033971972069033365, -0.049833672665746016, -0.00440830595213049, -0.0285312353973593, 0.008130980707560647, 0.009532910365475782, 0.07594895259281183, -0.0360898977566427, 0.004935803769604501, 0.044994615366270155, 0.0418614366719351, 0.007468988061556442, -0.017554697436681015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAyWdcHNTmIf"
      },
      "source": [
        "### Move embeddings to dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmSY4IvZTqtM",
        "outputId": "f44f471a-9523-4742-cc08-387d22f6a0b3"
      },
      "source": [
        "info2label = {}\n",
        "for i in range(len(labels)):\n",
        "    info2label[comment_infos[i]] = label[i]\n",
        "\n",
        "for key in info2label:\n",
        "    print(key, ':\\n', info2label[key])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Afghanistan', 'positive', 23) :\n",
            " [7.845292310613456, 4.628665314575266, 0.8785070598723065, -1.7030493792400954, 0.7023583855166133, 0.25813270612300493, -0.18706157316442965, -0.033971972069033365, -0.049833672665746016, -0.00440830595213049, -0.0285312353973593, 0.008130980707560647, 0.009532910365475782, 0.07594895259281183, -0.0360898977566427, 0.004935803769604501, 0.044994615366270155, 0.0418614366719351, 0.007468988061556442, -0.017554697436681015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAZsJbZFU6Nr"
      },
      "source": [
        "# 4. Creating Pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKPE4LjpVCYA",
        "outputId": "8aa0ec04-da4c-49ec-ef20-ce35bddb9d4d"
      },
      "source": [
        "features = {'time_idx': [], 'country': [], 'region': [], 'confirmed-cases': []}\n",
        "\n",
        "for log_type in ['positive', 'negative']:\n",
        "    for category in category_dic:\n",
        "        features[log_type + '-' + category] = []\n",
        "    for measure in measure_dic:\n",
        "        features[log_type + '-' + measure] = []\n",
        "    for i in range(COMMENT_DIMENSIONS):\n",
        "        features[log_type + '-comment-embedding' + str(i)] = []\n",
        "\n",
        "dataframe = pd.DataFrame(features)\n",
        "\n",
        "for country in tqdm(regulations):\n",
        "    for day in range(number_of_days):\n",
        "\n",
        "        new_entry = {}\n",
        "        new_entry['country'] = country\n",
        "        new_entry['time_idx'] = day\n",
        "        new_entry['region'] = country2region[country]\n",
        "        new_entry['confirmed-cases'] = time_series_confirmed_cases[country][day]\n",
        "\n",
        "        for log_type in ['positive', 'negative']:\n",
        "\n",
        "            for category in category_dic:\n",
        "                new_entry[log_type + '-' + category] = 0\n",
        "            for index in regulations[country][log_type][day]:\n",
        "                category = dataset['CATEGORY'][index]\n",
        "                new_entry[log_type + '-' + category] += 1\n",
        "\n",
        "            for measure in measure_dic:\n",
        "                new_entry[log_type + '-' + measure] = 0\n",
        "            for index in regulations[country][log_type][day]:\n",
        "                measure = dataset['MEASURE'][index]\n",
        "                new_entry[log_type + '-' + measure] += 1\n",
        "            \n",
        "            embedding = [0 for i in range(COMMENT_DIMENSIONS)]\n",
        "            if (country, log_type, day) in info2embedding:\n",
        "                embedding = info2embedding[(country, log_type, day)]\n",
        "            for i in range(COMMENT_DIMENSIONS):\n",
        "                new_entry[log_type + '-comment-embedding' + str(i)] = embedding[i]\n",
        "        \n",
        "        dataframe = dataframe.append(new_entry, ignore_index=True)\n",
        "\n",
        "print(dataframe.info())\n",
        "dataframe.to_excel('data.xlsx', sheet_name='main', index=False)\n",
        "dataframe.to_excel('/content/drive/MyDrive/DL-project/data.xlsx', sheet_name='main', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 175/175 [1:23:39<00:00, 28.68s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74200 entries, 0 to 74199\n",
            "Columns: 126 entries, time_idx to negative-comment-embedding19\n",
            "dtypes: float64(124), object(2)\n",
            "memory usage: 71.3+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-8ZeCKctsO_"
      },
      "source": [
        "# 5. Testing TFT usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvsAiNy7hvw"
      },
      "source": [
        "### Reading data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clf_hb25untG",
        "outputId": "8ac5475b-7ca5-4772-c370-3d79a4f4353f"
      },
      "source": [
        "xl_file = pd.ExcelFile('data.xlsx')\n",
        "\n",
        "sheets = {sheet_name: xl_file.parse(sheet_name) \n",
        "          for sheet_name in xl_file.sheet_names}\n",
        "\n",
        "data = sheets['main']\n",
        "\n",
        "print('\\n', dataframe.info(), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74200 entries, 0 to 74199\n",
            "Columns: 126 entries, time_idx to negative-comment-embedding19\n",
            "dtypes: float64(124), object(2)\n",
            "memory usage: 71.3+ MB\n",
            "\n",
            " None \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OSvmmaD7nPG"
      },
      "source": [
        "### Entering training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WbnzUv4RuV4v",
        "outputId": "26bcf276-b6a9-49ea-fdba-9423007bb118"
      },
      "source": [
        "max_prediction_length = 15\n",
        "max_encoder_length = 60\n",
        "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
        "\n",
        "feature_list = []\n",
        "for log_type in ['positive', 'negative']:\n",
        "    for category in category_dic:\n",
        "        feature_list.append(log_type + '-' + category)\n",
        "    for measure in measure_dic:\n",
        "        feature_list.append(log_type + '-' + measure) \n",
        "    for i in range(COMMENT_DIMENSIONS):\n",
        "        feature_list.append(log_type + '-comment-embedding' + str(i)) \n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    data[lambda x: x.time_idx <= training_cutoff],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"confirmed-cases\",\n",
        "    group_ids=[\"country\"],\n",
        "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"region\"],\n",
        "    static_reals=[],\n",
        "    time_varying_known_categoricals=[],  # We can further add information like \"special_days\" and \"month\"\n",
        "    variable_groups={},  # group of categorical variables can be treated as one variable\n",
        "    time_varying_known_reals=[\"time_idx\"],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=feature_list,\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        ")\n",
        "\n",
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
        "# for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
        "\n",
        "# create dataloaders for model\n",
        "batch_size = 128  # set this between 32 to 128\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/timeseries.py:517: UserWarning: Target scales will be only added for continous targets\n",
            "  warnings.warn(\"Target scales will be only added for continous targets\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 6057",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-57dfd8753c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# create validation set (predict=True) which means to predict the last max_prediction_length points in time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# for each series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_randomization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# create dataloaders for model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \"\"\"\n\u001b[1;32m   1099\u001b[0m         return cls.from_parameters(\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_randomization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_randomization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         )\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_parameters\u001b[0;34m(cls, parameters, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, dropout_categoricals, constant_fill_strategy, allow_missings, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Target normalizer is separate and not in scalers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNaNLabelEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m                 \u001b[0;31m# overwrite target because it requires encoding (continuous targets should not be normalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"__target__{self.target}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_forecasting/data/encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     raise KeyError(\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0;34mf\"Unknown category '{e.args[0]}' encountered. Set `add_nan=True` to allow unknown categories\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unknown category '6057' encountered. Set `add_nan=True` to allow unknown categories\""
          ]
        }
      ]
    }
  ]
}